---
@misc{vatai2024tadashienablingaibasedautomated,
      title={Tadashi: Enabling AI-Based Automated Code Generation With Guaranteed Correctness},
      author={Emil Vatai and Aleksandr Drozd and Ivan R. Ivanov and Yinghao Ren and Mohamed Wahib},
      year={2024},
      eprint={2410.03210},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.03210},

abbr = {arXiv},
},
@InProceedings{10.1007/978-3-031-72567-8_13,
author="Ivanov, Ivan R.
and Domke, Jens
and Endo, Toshio
and Doerfert, Johannes",
editor="Espinosa, Alexis
and Klemm, Michael
and de Supinski, Bronis R.
and Cytowski, Maciej
and Klinkenberg, Jannis",
title="Automatic Parallelization and OpenMP Offloading of Fortran Array Notation",
booktitle="Advancing OpenMP for Future Accelerators",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="197--209",
abstract="The Fortran programming language is prevalent in the scientific computing community with a wealth of existing software written in it. It is still being developed with the latest standard released in 2023. However, due to its long history, many old code bases are in need of modernization for new HPC systems. One advantage Fortran has over C and C++, which are other languages broadly used in scientific computing, is the easy syntax for manipulating entire arrays or subarrays. However, this feature is underused as there was no way of offloading them to accelerators and support for parallelization has been unsatisfactory. The new OpenMP 6.0 standard introduces the workdistribute directive which enables parallelization and/or offloading automatically by just annotating the region the programmer wishes to speed up. We implement workdistribute in the LLVM project's Fortran compiler, called Flang. Flang uses MLIR -- Multi-Level Intermediate Representation -- which allows for a structured representation that captures the high level semantics of array manipulation and OpenMP. This allows us to build an implementation that performs on par with more verbose manually parallelized OpenMP code. By offloading linear algebra operations to vendor libraries, we also enable software developers to easily unlock the full potential of their hardware without needing to write verbose, vendor-specific source code.",
isbn="978-3-031-72567-8",

pdf = {iwomp-workdistribute.pdf},
abbr = {IWOMP '24},
},
@InProceedings{10.1007/978-3-031-73370-3_1,
author="Burak, Semih
and Ivanov, Ivan R.
and Domke, Jens
and M{\"u}ller, Matthias",
editor="Blaas-Schenner, Claudia
and Niethammer, Christoph
and Haas, Tobias",
title="SPMD IR: Unifying SPMD and Multi-value IR Showcased for Static Verification of Collectives",
booktitle="Recent Advances in the Message Passing Interface",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="3--20",
abstract="To effectively utilize modern HPC clusters, inter-node communication and related single program, multiple data (SPMD) parallel programming models such as mpi are inevitable. Current tools and compilers that employ analyses of SPMD models often have the limitation of only supporting one model or implementing the necessary abstraction internally. This makes the analysis and effort for the abstraction neither reusable nor the tool extensible to other models without extensive changes to the tool itself.",
isbn="978-3-031-73370-3",

abbr = {EuroMPI '24},
},
@misc{ivanov2024inputgen,
title={Input-Gen: Guided Generation of Stateful Inputs for Testing, Tuning, and Training},
author={Ivan R. Ivanov and Joachim Meyer and Aiden Grossman and William S. Moses and Johannes Doerfert},
year={2024},
month={jun},
eprint={2406.08843},
archivePrefix={arXiv},
publisher = {arXiv},
abstract = {The size and complexity of software applications is increasing at an accelerating pace. Source code repositories (along with their dependencies) require vast amounts of labor to keep them tested, maintained, and up to date. As the discipline now begins to also incorporate automatically generated programs, automation in testing and tuning is required to keep up with the pace - let alone reduce the present level of complexity. While machine learning has been used to understand and generate code in various contexts, machine learning models themselves are trained almost exclusively on static code without inputs, traces, or other execution time information. This lack of training data limits the ability of these models to understand real-world problems in software. In this work we show that inputs, like code, can be generated automatically at scale. Our generated inputs are stateful, and appear to faithfully reproduce the arbitrary data structures and system calls required to rerun a program function. By building our tool within the compiler, it both can be applied to arbitrary programming languages and architectures and can leverage static analysis and transformations for improved performance. Our approach is able to produce valid inputs, including initial memory states, for 90% of the ComPile dataset modules we explored, for a total of 21.4 million executable functions. Further, we find that a single generated input results in an average block coverage of 37%, whereas guided generation of five inputs improves it to 45%. },
primaryClass={id='cs.SE' full_name='Software Engineering' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers design tools, software metrics, testing and debugging, programming environments, etc. Roughly includes material in all of ACM Subject Classes D.2, except that D.2.4 (program verification) should probably have Logics in Computer Science as the primary subject area.'},

selected={true},
pdf = {input-gen-arxiv.pdf},
abbr = {arXiv},
},
@INPROCEEDINGS {polygeist-coarsening,
author = {I. R. Ivanov and O. Zinenko and J. Domke and T. Endo and W. S. Moses},
booktitle = {2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
title = {Retargeting and Respecializing GPU Workloads for Performance Portability},
year = {2024},
volume = {},
issn = {},
pages = {119-132},
abstract = {In order to come close to peak performance, accelerators like GPUs require significant architecture-specific tuning that understand the availability of shared memory, parallelism, tensor cores, etc. Unfortunately, the pursuit of higher performance and lower costs have led to a significant diversification of architecture designs, even from the same vendor. This creates the need for performance portability across different GPUs, especially important for programs in a particular programming model with a certain architecture in mind. Even when the program can be seamlessly executed on a different architecture, it may suffer a performance penalty due to it not being sized appropriately to the available hardware resources such as fast memory and registers, let alone not using newer advanced features of the architecture. We propose a new approach to improving performance of (legacy) CUDA programs for modern machines by automatically adjusting the amount of work each parallel thread does, and the amount of memory and register resources it requires. By operating within the MLIR compiler infrastructure, we are able to also target AMD GPUs by performing automatic translation from CUDA and simultaneously adjust the program granularity to fit the size of target GPUs. Combined with autotuning assisted by the platform-specific compiler, our approach demonstrates 27% geomean speedup on the Rodinia benchmark suite over baseline CUDA implementation as well as performance parity between similar NVIDIA and AMD GPUs executing the same CUDA program.},
keywords = {costs;codes;memory management;graphics processing units;hardware;software;registers},
doi = {10.1109/CGO57630.2024.10444828},
url = {https://doi.ieeecomputersociety.org/10.1109/CGO57630.2024.10444828},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar},

selected={true},
pdf = {polygeist-gpu-cgo24-preprint.pdf},
abbr = {CGO '24},
},
@inproceedings{polygeist-gpu-to-cpu,
author = {Moses, William S. and Ivanov, Ivan R. and Domke, Jens and Endo, Toshio and Doerfert, Johannes and Zinenko, Oleksandr},
title = {High-Performance GPU-to-CPU Transpilation and Optimization via High-Level Parallel Constructs},
year = {2023},
isbn = {9798400700156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572848.3577475},
doi = {10.1145/3572848.3577475},
abstract = {While parallelism remains the main source of performance, architectural implementations and programming models change with each new hardware generation, often leading to costly application re-engineering. Most tools for performance portability require manual and costly application porting to yet another programming model.We propose an alternative approach that automatically translates programs written in one programming model (CUDA), into another (CPU threads) based on Polygeist/MLIR. Our approach includes a representation of parallel constructs that allows conventional compiler transformations to apply transparently and without modification and enables parallelism-specific optimizations. We evaluate our framework by transpiling and optimizing the CUDA Rodinia benchmark suite for a multi-core CPU and achieve a 58% geomean speedup over handwritten OpenMP code. Further, we show how CUDA kernels from PyTorch can efficiently run and scale on the CPU-only Supercomputer Fugaku without user intervention. Our PyTorch compatibility layer making use of transpiled CUDA PyTorch kernels outperforms the PyTorch CPU native backend by 2.7\texttimes{}.},
booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {119–134},
numpages = {16},
keywords = {MLIR, polygeist, CUDA, barrier synchronization},
location = {Montreal, QC, Canada},
series = {PPoPP '23},

abbr = {PPoPP '23},
pdf = {polygeist-ppopp23-final.pdf},
},
@inproceedings{hyperx,
author = {Domke, Jens and Matsuoka, Satoshi and Ivanov, Ivan R. and Tsushima, Yuki and Yuki, Tomoya and Nomura, Akihiro and Miura, Shin'ichi and McDonald, Nie and Floyd, Dennis L. and Dub\'{e}, Nicolas},
title = {HyperX Topology: First at-Scale Implementation and Comparison to the Fat-Tree},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356140},
doi = {10.1145/3295500.3356140},
abstract = {The de-facto standard topology for modern HPC systems and data-centers are Folded Clos networks, commonly known as Fat-Trees. The number of network endpoints in these systems is steadily increasing. The switch radix increase is not keeping up, forcing an increased path length in these multi-level trees that will limit gains for latency-sensitive applications. Additionally, today's Fat-Trees force the extensive use of active optical cables which carries a prohibitive cost-structure at scale. To tackle these issues, researchers proposed various low-diameter topologies, such as Dragonfly. Another novel, but only theoretically studied, option is the HyperX. We built the world's first 3 Pflop/s supercomputer with two separate networks, a 3--level Fat-Tree and a 12\texttimes{}8 HyperX. This dual-plane system allows us to perform a side-by-side comparison using a broad set of benchmarks. We show that the HyperX, together with our novel communication pattern-aware routing, can challenge the performance of, or even outperform, traditional Fat-Trees.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {40},
numpages = {23},
keywords = {network topology, fat-tree, PARX, InfiniBand, HyperX, routing},
location = {Denver, Colorado},
series = {SC '19},

abbr = {SC '19},
},
@inproceedings{raptor,
title={RAPTOR: Practical Numerical Profiling of Scientific Applications},
author={Faveo Hoerold and Ivan R. Ivanov and Akash Dhruv and William S. Moses and Anshu Dubey and Mohamed Wahib and Jens Domke},
year={2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url={https://arxiv.org/abs/2507.04647},

abbr = {SC '25},
}
@InProceedings{dynamic-thread-coarsening,
author="Ivanov, Ivan R.
and Domke, Jens
and Endo, Toshio
and Doerfert, Johannes",
title="Dynamic Thread Coarsening for CPU and GPU OpenMP Code",
year="2025",

abbr = {LLVM-HPC @ SC '25},
},
---
