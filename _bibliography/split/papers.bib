---
@INPROCEEDINGS {polygeist-coarsening,
author = {Ivan R. Ivanov and O. Zinenko and J. Domke and T. Endo and W. S. Moses},
booktitle = {2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
title = {Retargeting and Respecializing GPU Workloads for Performance Portability},
year = {2024},
volume = {},
issn = {},
pages = {119-132},
abstract = {},
keywords = {costs;codes;memory management;graphics processing units;hardware;software;registers},
doi = {10.1109/CGO57630.2024.10444828},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar},

selected={true},
pdf = {polygeist-gpu-cgo24-preprint.pdf},
abbr = {CGO '24},
},
@inproceedings{polygeist-gpu-to-cpu,
author = {Moses, William S. and Ivanov, Ivan R. and Domke, Jens and Endo, Toshio and Doerfert, Johannes and Zinenko, Oleksandr},
title = {High-Performance GPU-to-CPU Transpilation and Optimization via High-Level Parallel Constructs},
year = {2023},
isbn = {9798400700156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3572848.3577475},
abstract = {},
booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {119â€“134},
numpages = {16},
keywords = {MLIR, polygeist, CUDA, barrier synchronization},
location = {Montreal, QC, Canada},
series = {PPoPP '23},

abbr = {PPoPP '23},
pdf = {polygeist-ppopp23-final.pdf},
},
@inproceedings{hyperx,
author = {Domke, Jens and Matsuoka, Satoshi and Ivanov, Ivan R. and Tsushima, Yuki and Yuki, Tomoya and Nomura, Akihiro and Miura, Shin'ichi and McDonald, Nie and Floyd, Dennis L. and Dub\'{e}, Nicolas},
title = {HyperX Topology: First at-Scale Implementation and Comparison to the Fat-Tree},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3295500.3356140},
abstract = {The de-facto standard topology for modern HPC systems and data-centers are Folded Clos networks, commonly known as Fat-Trees. The number of network endpoints in these systems is steadily increasing. The switch radix increase is not keeping up, forcing an increased path length in these multi-level trees that will limit gains for latency-sensitive applications. Additionally, today's Fat-Trees force the extensive use of active optical cables which carries a prohibitive cost-structure at scale. To tackle these issues, researchers proposed various low-diameter topologies, such as Dragonfly. Another novel, but only theoretically studied, option is the HyperX. We built the world's first 3 Pflop/s supercomputer with two separate networks, a 3--level Fat-Tree and a 12\texttimes{}8 HyperX. This dual-plane system allows us to perform a side-by-side comparison using a broad set of benchmarks. We show that the HyperX, together with our novel communication pattern-aware routing, can challenge the performance of, or even outperform, traditional Fat-Trees.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {40},
numpages = {23},
keywords = {network topology, fat-tree, PARX, InfiniBand, HyperX, routing},
location = {Denver, Colorado},
series = {SC '19},

abbr = {SC '19},
},
---
